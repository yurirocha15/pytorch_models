{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = 256\n",
    "batch_test = 1\n",
    "epochs = 100\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(anomaly, batch_train, batch_test):\n",
    "    path = './data/mnist'\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    #import the original dataset\n",
    "    orig_train_dataset = MNIST(path, train=True, download=True, transform=transform)\n",
    "    orig_test_dataset = MNIST(path, train=False, download=True, transform=transform)\n",
    "\n",
    "    #remove the anomaly from the train dataset\n",
    "    idx = orig_train_dataset.train_labels != anomaly\n",
    "    train_dataset = dataset.Subset(orig_train_dataset, np.where(idx==True)[0])\n",
    "    #split the test set into normal and anomaly dataset\n",
    "    idx = orig_test_dataset.test_labels != anomaly\n",
    "    test_dataset = dataset.Subset(orig_test_dataset, np.where(idx==True)[0])\n",
    "    anomaly_test_dataset = dataset.Subset(orig_test_dataset, np.where(idx==False)[0])\n",
    "\n",
    "    #create dataloaders\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_train, shuffle=True, num_workers=12)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_test, shuffle=False, num_workers=12, drop_last=True)\n",
    "    anomaly_dl = DataLoader(anomaly_test_dataset, batch_size=batch_test, shuffle=False, num_workers=12, drop_last=True)\n",
    "\n",
    "    return train_dl, test_dl, anomaly_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_Loss(x, x_hat, mu, logvariance):\n",
    "    '''\n",
    "    The original reconstruction loss used by Kingma and Welling was the cross-entropy log(p(x|z)). \n",
    "    As p(x|z) ~ N(mu, sigma), the cross-entropy loss is proportional to the MSE loss between the input and the output.\n",
    "    https://www.deeplearningbook.org/contents/ml.html (page 130)\n",
    "    '''\n",
    "    mse_loss = nn.functional.mse_loss(x, x_hat)\n",
    "    '''\n",
    "    The KL Divergence between N(mu, sigma) and N(0, 1) is \n",
    "    0.5 * sum_over_batch(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    For sigma values close to 0, the output of log(sigma^2) can explode\n",
    "    Thus, we learn log(sigma^2) instead (logvariance)\n",
    "    '''\n",
    "    KL_loss = -0.5 * torch.sum(1 + logvariance - mu ** 2 - logvariance.exp())\n",
    "\n",
    "    return KL_loss + mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Cell(nn.Module):\n",
    "    '''\n",
    "    @input:\n",
    "        input_size: number of filters of the input tensor\n",
    "        n_filters: number of filters of the output tensor\n",
    "    @output:\n",
    "        tensor with size = (batch_size, n_filters, X.size()[2]/2, X.size()[3]/2)\n",
    "    '''\n",
    "    def __init__(self, input_size, n_filters):\n",
    "        super(Conv_Cell, self).__init__()\n",
    "        conv = nn.Conv2d(input_size, n_filters, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        nn.init.xavier_uniform_(conv.weight)\n",
    "        self.layers = nn.Sequential(\n",
    "            conv,\n",
    "            nn.BatchNorm2d(n_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "class Deconv_Cell(nn.Module):\n",
    "    '''\n",
    "    @input:\n",
    "        input_size: number of filters of the input tensor\n",
    "        n_filters: number of filters of the output tensor\n",
    "    @output:\n",
    "        tensor with size = (batch_size, n_filters, X.size()[2]*2, X.size()[3]*2)\n",
    "    '''\n",
    "    def __init__(self, input_size, n_filters):\n",
    "        super(Deconv_Cell, self).__init__()\n",
    "        deconv = nn.ConvTranspose2d(input_size, n_filters, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        nn.init.xavier_uniform_(deconv.weight)\n",
    "        self.layers = nn.Sequential(\n",
    "            deconv,\n",
    "            nn.BatchNorm2d(n_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, X):\n",
    "        return X.view(X.size()[0], -1)\n",
    "\n",
    "class Deflatten(nn.Module):\n",
    "    '''\n",
    "    @input:\n",
    "        out_size: height/width of the output tensor\n",
    "        n_filters: number of filters of the output tensor\n",
    "    @output:\n",
    "        tensor with size = (batch_size, n_filters, out_size, out_size)\n",
    "    '''\n",
    "    def __init__(self, out_size, n_filters):\n",
    "        super(Deflatten, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X.view(X.size()[0], self.n_filters, self.out_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential( #input 28x28x1\n",
    "            Conv_Cell(input_size, 16), #outsize 14x14x16\n",
    "            Conv_Cell(16, 32), #outsize 7x7x32\n",
    "            Flatten() #outsize 1568\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(1568, 100)\n",
    "        self.fc_logvariance = nn.Linear(1568, 100)\n",
    "\n",
    "\n",
    "        deconv = nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        nn.init.xavier_uniform_(deconv.weight)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 1568), #outsize 1568\n",
    "            Deflatten(7, 32), #outsize 7x7x32\n",
    "            Deconv_Cell(32, 16), #outsize 14x14x16\n",
    "            deconv #outsize 28x28x1\n",
    "        )\n",
    "\n",
    "    def sample_latent_vector(self, mu, logvariance):\n",
    "        samples = torch.randn_like(mu)\n",
    "        samples.to(device)\n",
    "        '''\n",
    "        logvariance = log(sigma^2) -> sigma = exp(logvariance/2)\n",
    "        '''\n",
    "        return mu + samples * (torch.exp(0.5*logvariance))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        mu = self.fc_mu(X)\n",
    "        logvariance = self.fc_logvariance(X)\n",
    "        X = self.sample_latent_vector(mu, logvariance)\n",
    "        X = self.decoder(X)\n",
    "        return X, mu, logvariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_dl, anomaly_dl):\n",
    "    model = VAE()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    _, (anomaly_fixed_test, labels) = enumerate(anomaly_dl).__next__()\n",
    "    anomaly_fixed_test = anomaly_fixed_test.to(device)\n",
    "\n",
    "    training_losses = []\n",
    "    anomaly_losses = []\n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        total_loss=0.0\n",
    "        anomaly_loss=0.0\n",
    "        for i, data in enumerate(train_dl):\n",
    "            x, labels = data\n",
    "            x = x.to(device)\n",
    "            #initialize gradients\n",
    "            optimizer.zero_grad()\n",
    "            #forward pass\n",
    "            x_hat, mu, logvariance = model(x)\n",
    "            #calculate loss\n",
    "            loss = VAE_Loss(x, x_hat, mu, logvariance)\n",
    "            #gradient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        #check how the anomaly loss change during training\n",
    "        with torch.no_grad():\n",
    "            x_hat, mu, logvariance = model(anomaly_fixed_test)\n",
    "            loss = VAE_Loss(anomaly_fixed_test, x_hat, mu, logvariance)\n",
    "            anomaly_loss += loss.item()\n",
    "        \n",
    "        training_losses.append(total_loss/len(train_dl))\n",
    "        anomaly_losses.append(anomaly_loss)\n",
    "\n",
    "        tqdm.write(f'Loss: {total_loss/len(train_dl)} \\t Anomaly Loss: {anomaly_loss}')\n",
    "    return model, training_losses, anomaly_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epochs', style=ProgressStyle(description_width='initial')…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d54c84fa7e849ab9434a57e6596b388"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loss: 19.68285060436768 \t Anomaly Loss: 38.02560806274414\nLoss: 2.3900828946976973 \t Anomaly Loss: 16.114274978637695\nLoss: 1.2035951784376562 \t Anomaly Loss: 8.485692024230957\nLoss: 0.7964525114855118 \t Anomaly Loss: 5.166813373565674\nLoss: 0.6064379479405443 \t Anomaly Loss: 3.4739675521850586\nLoss: 0.4975477078609918 \t Anomaly Loss: 2.272233486175537\nLoss: 0.43076125214085775 \t Anomaly Loss: 1.8001391887664795\nLoss: 0.38760357005356333 \t Anomaly Loss: 1.521540880203247\nLoss: 0.35775885733627005 \t Anomaly Loss: 1.0550117492675781\nLoss: 0.3346467215986647 \t Anomaly Loss: 0.9051533937454224\nLoss: 0.3201422388031638 \t Anomaly Loss: 0.8537915945053101\nLoss: 0.3072014921515651 \t Anomaly Loss: 0.6584595441818237\nLoss: 0.2994239777150239 \t Anomaly Loss: 0.6771160364151001\nLoss: 0.29357749442377035 \t Anomaly Loss: 0.6062570810317993\nLoss: 0.2895015122975118 \t Anomaly Loss: 0.8280441761016846\nLoss: 0.2860493386285545 \t Anomaly Loss: 0.6662386655807495\nLoss: 0.285025578014244 \t Anomaly Loss: 0.5769940614700317\nLoss: 0.2825553403274547 \t Anomaly Loss: 0.6115537881851196\nLoss: 0.2804991694949788 \t Anomaly Loss: 0.5655550360679626\nLoss: 0.27979528741723686 \t Anomaly Loss: 0.8302958011627197\nLoss: 0.2781188471782842 \t Anomaly Loss: 0.5109569430351257\nLoss: 0.27802901149854153 \t Anomaly Loss: 0.6012930870056152\nLoss: 0.2774460227002759 \t Anomaly Loss: 0.47500357031822205\nLoss: 0.27506561206995384 \t Anomaly Loss: 0.5093429088592529\nLoss: 0.27567548739486897 \t Anomaly Loss: 0.4236394166946411\nLoss: 0.273798100218265 \t Anomaly Loss: 0.7093554735183716\nLoss: 0.2737288682418462 \t Anomaly Loss: 0.6468952894210815\nLoss: 0.27214059399429863 \t Anomaly Loss: 0.4360601007938385\nLoss: 0.2739829177863499 \t Anomaly Loss: 0.5552881956100464\nLoss: 0.2730860996352145 \t Anomaly Loss: 0.5091468691825867\nLoss: 0.27046859897452696 \t Anomaly Loss: 0.46472886204719543\nLoss: 0.27210789573263133 \t Anomaly Loss: 0.7695176601409912\nLoss: 0.2722591675597535 \t Anomaly Loss: 0.497293084859848\nLoss: 0.2705150219992068 \t Anomaly Loss: 0.5782693028450012\nLoss: 0.271142719606676 \t Anomaly Loss: 0.40342000126838684\nLoss: 0.2695369005379592 \t Anomaly Loss: 0.4117235839366913\nLoss: 0.27009332964758903 \t Anomaly Loss: 0.4320738911628723\nLoss: 0.26989869115620674 \t Anomaly Loss: 0.5047953128814697\nLoss: 0.26919220299763086 \t Anomaly Loss: 0.46844717860221863\nLoss: 0.2688314746470141 \t Anomaly Loss: 0.4441024363040924\nLoss: 0.2685773238804213 \t Anomaly Loss: 0.4134642481803894\nLoss: 0.26906457977182063 \t Anomaly Loss: 0.4086781442165375\nLoss: 0.2688033049099544 \t Anomaly Loss: 0.4428763687610626\nLoss: 0.2685360534833028 \t Anomaly Loss: 0.4138655662536621\nLoss: 0.2682809681758373 \t Anomaly Loss: 0.45140400528907776\nLoss: 0.2682205446196731 \t Anomaly Loss: 0.3930797576904297\nLoss: 0.2682578601075347 \t Anomaly Loss: 0.38653799891471863\nLoss: 0.2674593673302577 \t Anomaly Loss: 0.41114988923072815\nLoss: 0.26748051310079335 \t Anomaly Loss: 0.3860948979854584\nLoss: 0.2670851417547147 \t Anomaly Loss: 0.39803561568260193\nLoss: 0.26740626668083595 \t Anomaly Loss: 0.3856133818626404\nLoss: 0.26677552548033245 \t Anomaly Loss: 0.3757093548774719\nLoss: 0.26591724919144216 \t Anomaly Loss: 0.39304327964782715\nLoss: 0.267637095631227 \t Anomaly Loss: 0.3874184191226959\nLoss: 0.2671407161203362 \t Anomaly Loss: 0.4220591187477112\nLoss: 0.26574165470148686 \t Anomaly Loss: 0.3891802430152893\nLoss: 0.2668594713747149 \t Anomaly Loss: 0.4044012427330017\nLoss: 0.2664327564676838 \t Anomaly Loss: 0.4093576967716217\nLoss: 0.26668732802896106 \t Anomaly Loss: 0.4583428204059601\nLoss: 0.26551577577576835 \t Anomaly Loss: 0.4182368218898773\nLoss: 0.26563861881487466 \t Anomaly Loss: 0.44458842277526855\nLoss: 0.2657976878641625 \t Anomaly Loss: 0.3686943054199219\nLoss: 0.26576671651481876 \t Anomaly Loss: 0.37779679894447327\nLoss: 0.26596400758923866 \t Anomaly Loss: 0.38600900769233704\nLoss: 0.26503330354507154 \t Anomaly Loss: 0.38800907135009766\nLoss: 0.26441437545612717 \t Anomaly Loss: 0.46388518810272217\nLoss: 0.2664206973547061 \t Anomaly Loss: 0.390261709690094\nLoss: 0.265137672653565 \t Anomaly Loss: 0.3988732397556305\nLoss: 0.2649802844728944 \t Anomaly Loss: 0.370662122964859\nLoss: 0.26491677108601003 \t Anomaly Loss: 0.3784599304199219\nLoss: 0.2648990931771916 \t Anomaly Loss: 0.37192678451538086\nLoss: 0.2649587160031471 \t Anomaly Loss: 0.4061986207962036\nLoss: 0.26443471534717716 \t Anomaly Loss: 0.4798640012741089\nLoss: 0.26512815506147913 \t Anomaly Loss: 0.3953210413455963\nLoss: 0.2648097956674339 \t Anomaly Loss: 0.36522701382637024\nLoss: 0.26446926062981757 \t Anomaly Loss: 0.39339736104011536\nLoss: 0.264051151628325 \t Anomaly Loss: 0.3713756799697876\nLoss: 0.26398596966407706 \t Anomaly Loss: 0.3758818209171295\nLoss: 0.2641947662336587 \t Anomaly Loss: 0.3882775604724884\nLoss: 0.2641098663122696 \t Anomaly Loss: 0.4210781455039978\nLoss: 0.2642954605747257 \t Anomaly Loss: 0.38140276074409485\nLoss: 0.26408937324786325 \t Anomaly Loss: 0.39670103788375854\nLoss: 0.2636691317755795 \t Anomaly Loss: 0.3654741644859314\nLoss: 0.2641514838094542 \t Anomaly Loss: 0.3809988498687744\nLoss: 0.2640293533632741 \t Anomaly Loss: 0.3995878994464874\nLoss: 0.26420581370768464 \t Anomaly Loss: 0.39837580919265747\nLoss: 0.2637514476240034 \t Anomaly Loss: 0.3770889341831207\nLoss: 0.26330856354631615 \t Anomaly Loss: 0.392065167427063\nLoss: 0.2630367949164125 \t Anomaly Loss: 0.3670923113822937\nLoss: 0.2647525899509001 \t Anomaly Loss: 0.5895885229110718\nLoss: 0.26315378222239794 \t Anomaly Loss: 0.3651350438594818\nLoss: 0.26268496608593056 \t Anomaly Loss: 0.41628211736679077\nLoss: 0.2639006923641679 \t Anomaly Loss: 0.3675205111503601\nLoss: 0.2622737873588088 \t Anomaly Loss: 0.4028121829032898\nLoss: 0.2634991840674327 \t Anomaly Loss: 0.43409642577171326\nLoss: 0.2633100754939593 \t Anomaly Loss: 0.4008738100528717\nLoss: 0.26282801016195284 \t Anomaly Loss: 0.45579972863197327\nLoss: 0.2624835496470773 \t Anomaly Loss: 0.372798353433609\nLoss: 0.26386555040023735 \t Anomaly Loss: 0.39568114280700684\nLoss: 0.2618326869060302 \t Anomaly Loss: 0.3970688283443451\n\n"
    }
   ],
   "source": [
    "#One class as anomaly, the others as normal classes\n",
    "anomaly = 0\n",
    "train_dl, test_dl, anomaly_dl = load_dataset(anomaly, batch_train, batch_test)\n",
    "model0, training_losses0, anomaly_losses0 = train(train_dl, anomaly_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epochs', style=ProgressStyle(description_width='initial')…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6b8ef48422240fc9b16dfc49a12bdbf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loss: 23.195323292089967 \t Anomaly Loss: 45.687416076660156\nLoss: 3.032850387145062 \t Anomaly Loss: 15.357585906982422\nLoss: 1.4974287167674496 \t Anomaly Loss: 7.545223236083984\nLoss: 0.96065728908398 \t Anomaly Loss: 4.646596908569336\nLoss: 0.710412855516104 \t Anomaly Loss: 3.571070671081543\nLoss: 0.5769590957611263 \t Anomaly Loss: 3.0875260829925537\nLoss: 0.4952808292973943 \t Anomaly Loss: 1.9687163829803467\nLoss: 0.43875918676014564 \t Anomaly Loss: 1.7380006313323975\nLoss: 0.4021780289450184 \t Anomaly Loss: 1.5403180122375488\nLoss: 0.3764853941506985 \t Anomaly Loss: 1.3360602855682373\nLoss: 0.3550921660296771 \t Anomaly Loss: 1.2199890613555908\nLoss: 0.3403446174540869 \t Anomaly Loss: 0.8661714792251587\nLoss: 0.3279375338396963 \t Anomaly Loss: 0.8733000755310059\nLoss: 0.32006992827228853 \t Anomaly Loss: 1.1052120923995972\nLoss: 0.31323402637050074 \t Anomaly Loss: 0.7423142790794373\nLoss: 0.3079343496822939 \t Anomaly Loss: 0.9276999235153198\nLoss: 0.3053715771057454 \t Anomaly Loss: 1.230007529258728\nLoss: 0.3031738752505931 \t Anomaly Loss: 0.6709399819374084\nLoss: 0.2987151481524235 \t Anomaly Loss: 0.7249859571456909\nLoss: 0.29939258871387603 \t Anomaly Loss: 0.736244797706604\nLoss: 0.297856215329445 \t Anomaly Loss: 0.9188253879547119\nLoss: 0.2979098534455248 \t Anomaly Loss: 0.7338877320289612\nLoss: 0.2949958997828906 \t Anomaly Loss: 0.9343674182891846\nLoss: 0.2945315611033308 \t Anomaly Loss: 1.1128860712051392\nLoss: 0.2940082383685324 \t Anomaly Loss: 0.7949069142341614\nLoss: 0.2942894916145169 \t Anomaly Loss: 0.4837099313735962\nLoss: 0.29251721959297255 \t Anomaly Loss: 1.0304622650146484\nLoss: 0.2936520635509262 \t Anomaly Loss: 0.4420138895511627\nLoss: 0.29257803121391607 \t Anomaly Loss: 0.5965046882629395\nLoss: 0.2924237955375021 \t Anomaly Loss: 0.7117833495140076\nLoss: 0.29165655091887904 \t Anomaly Loss: 0.6679496765136719\nLoss: 0.2903135147343735 \t Anomaly Loss: 0.5051254034042358\nLoss: 0.2912203912832299 \t Anomaly Loss: 0.6013844609260559\nLoss: 0.29013236122829716 \t Anomaly Loss: 0.5501623153686523\nLoss: 0.29112905517918153 \t Anomaly Loss: 0.543182373046875\nLoss: 0.2888234541768215 \t Anomaly Loss: 0.7590170502662659\nLoss: 0.28835368549981183 \t Anomaly Loss: 0.5232723951339722\nLoss: 0.2904978623839558 \t Anomaly Loss: 0.3151147961616516\nLoss: 0.289276046161892 \t Anomaly Loss: 0.6448580026626587\nLoss: 0.28758147877662266 \t Anomaly Loss: 0.3600728511810303\nLoss: 0.2886589755531119 \t Anomaly Loss: 0.6015106439590454\nLoss: 0.28799127863377944 \t Anomaly Loss: 0.5596513748168945\nLoss: 0.2884914779505667 \t Anomaly Loss: 0.33701109886169434\nLoss: 0.2872982186143424 \t Anomaly Loss: 0.5048643350601196\nLoss: 0.28774441794759514 \t Anomaly Loss: 0.44387978315353394\nLoss: 0.28713287704703616 \t Anomaly Loss: 0.34190940856933594\nLoss: 0.28798403706298725 \t Anomaly Loss: 0.41154447197914124\nLoss: 0.2866814145449401 \t Anomaly Loss: 0.6452130079269409\nLoss: 0.28628529230801286 \t Anomaly Loss: 0.4070863723754883\nLoss: 0.28666521420999735 \t Anomaly Loss: 0.4533379077911377\nLoss: 0.28704958501197 \t Anomaly Loss: 0.30848240852355957\nLoss: 0.28535750565551765 \t Anomaly Loss: 0.3355998992919922\nLoss: 0.2865713853008893 \t Anomaly Loss: 0.3052528202533722\nLoss: 0.28588126669554004 \t Anomaly Loss: 0.4194100499153137\nLoss: 0.2861434711413939 \t Anomaly Loss: 0.6066440939903259\nLoss: 0.28605168907582257 \t Anomaly Loss: 0.37073010206222534\nLoss: 0.28576149545511564 \t Anomaly Loss: 0.3604946732521057\nLoss: 0.28536932036226964 \t Anomaly Loss: 0.2825382947921753\nLoss: 0.28410110832834873 \t Anomaly Loss: 0.33638155460357666\nLoss: 0.2851420563309133 \t Anomaly Loss: 0.27900075912475586\nLoss: 0.2849383861864028 \t Anomaly Loss: 0.31418144702911377\nLoss: 0.28459639097748396 \t Anomaly Loss: 0.4231838583946228\nLoss: 0.28476457804047905 \t Anomaly Loss: 0.31912535429000854\nLoss: 0.28441524212243036 \t Anomaly Loss: 0.40059804916381836\nLoss: 0.2846259497293905 \t Anomaly Loss: 0.3731529116630554\nLoss: 0.28424209460061567 \t Anomaly Loss: 0.46455496549606323\nLoss: 0.284703519808001 \t Anomaly Loss: 0.29865318536758423\nLoss: 0.2844586151809681 \t Anomaly Loss: 0.31638601422309875\nLoss: 0.2837267316976229 \t Anomaly Loss: 0.28636589646339417\nLoss: 0.2832869132276343 \t Anomaly Loss: 0.28376156091690063\nLoss: 0.2838763629331165 \t Anomaly Loss: 0.3315872251987457\nLoss: 0.2836192035589184 \t Anomaly Loss: 0.31682202219963074\nLoss: 0.28381347935311363 \t Anomaly Loss: 0.3027840852737427\nLoss: 0.28352867036449664 \t Anomaly Loss: 0.41360634565353394\nLoss: 0.28364272560535214 \t Anomaly Loss: 0.3047909736633301\nLoss: 0.28299192928609584 \t Anomaly Loss: 0.2958160638809204\nLoss: 0.2835403546995046 \t Anomaly Loss: 0.2703637480735779\nLoss: 0.2829021320265739 \t Anomaly Loss: 0.31271928548812866\nLoss: 0.2829639642250066 \t Anomaly Loss: 0.278104305267334\nLoss: 0.28275320756764544 \t Anomaly Loss: 0.27296003699302673\nLoss: 0.282932821012774 \t Anomaly Loss: 0.29691246151924133\nLoss: 0.28229472681540113 \t Anomaly Loss: 0.25002551078796387\nLoss: 0.28274645871904286 \t Anomaly Loss: 0.28926849365234375\nLoss: 0.2823603851239936 \t Anomaly Loss: 0.28312283754348755\nLoss: 0.2824465908995625 \t Anomaly Loss: 0.3164037764072418\nLoss: 0.2821773912797884 \t Anomaly Loss: 0.37639743089675903\nLoss: 0.2825951892860225 \t Anomaly Loss: 0.266684353351593\nLoss: 0.2820197813937358 \t Anomaly Loss: 0.2979511618614197\nLoss: 0.28176942551765694 \t Anomaly Loss: 0.2512267827987671\nLoss: 0.28212107025704036 \t Anomaly Loss: 0.2570294439792633\nLoss: 0.2817647424565644 \t Anomaly Loss: 0.2836896777153015\nLoss: 0.2816287233334343 \t Anomaly Loss: 0.24625256657600403\nLoss: 0.28230379266040523 \t Anomaly Loss: 0.2966846823692322\nLoss: 0.281427610965193 \t Anomaly Loss: 0.2770389914512634\nLoss: 0.2821371584486704 \t Anomaly Loss: 0.30975502729415894\nLoss: 0.2812552131524607 \t Anomaly Loss: 0.2759283185005188\nLoss: 0.2809020098374814 \t Anomaly Loss: 0.27336838841438293\nLoss: 0.28136080857657014 \t Anomaly Loss: 0.27183765172958374\nLoss: 0.281069578242903 \t Anomaly Loss: 0.2491176873445511\nLoss: 0.28110137445872285 \t Anomaly Loss: 0.27098995447158813\n\n"
    }
   ],
   "source": [
    "#One class as anomaly, the others as normal classes\n",
    "anomaly = 1\n",
    "train_dl, test_dl, anomaly_dl = load_dataset(anomaly, batch_train, batch_test)\n",
    "model1, training_losses1, anomaly_losses1 = train(train_dl, anomaly_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.25726127814290656\n"
    }
   ],
   "source": [
    "anomaly = 0\n",
    "train_dl = load_dataset(anomaly, batch_train, 512)[0]\n",
    "mse_normal = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, (x, label) in enumerate(train_dl):\n",
    "        x = x.to(device)\n",
    "        x_hat = model0(x)[0]\n",
    "        mse = nn.functional.mse_loss(x, x_hat)\n",
    "        mse_normal += mse.item()\n",
    "    mse_normal /= len(train_dl)\n",
    "print(mse_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.27666353109920994\n"
    }
   ],
   "source": [
    "anomaly = 1\n",
    "train_dl = load_dataset(anomaly, batch_train, batch_test)[0]\n",
    "mse_normal = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, (x, label) in enumerate(train_dl):\n",
    "        x = x.to(device)\n",
    "        x_hat = model1(x)[0]\n",
    "        mse = nn.functional.mse_loss(x, x_hat)\n",
    "        mse_normal += mse.item()\n",
    "    mse_normal /= len(train_dl)\n",
    "print(mse_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Using the reconstruction error as a classifier for now.\n",
    "\n",
    "Might try implementing the reconstruction probability later. \n",
    "http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def calculate_auroc(model, anomaly, thresh):\n",
    "    model.eval()\n",
    "    model.to(\"cpu\")\n",
    "    _, normal_dl, anomaly_dl = load_dataset(anomaly, 1, 512)\n",
    "    with torch.no_grad():\n",
    "        y = np.empty(shape=(len(normal_dl)+len(anomaly_dl))*512)\n",
    "        labels = np.empty(shape=(len(normal_dl)+len(anomaly_dl))*512)\n",
    "        for i, (x, label) in enumerate(normal_dl):\n",
    "            x_hat = model(x)[0]\n",
    "            mse = np.square(x.numpy() - x_hat.numpy())\n",
    "            mse = mse.reshape((mse.shape[0], -1)).mean(axis=1)\n",
    "            y[i*512 : (i+1)*512] = (mse > thresh)\n",
    "            labels[i*512 : (i+1)*512] = False\n",
    "        offset = len(normal_dl)*512\n",
    "        for i, (x, label) in enumerate(anomaly_dl):\n",
    "            x_hat = model(x)[0]\n",
    "            mse = np.square(x.numpy() - x_hat.numpy())\n",
    "            mse = mse.reshape((mse.shape[0], -1)).mean(axis=1)\n",
    "            y[offset+i*512 : offset+(i+1)*512] = (mse > thresh)\n",
    "            labels[offset+i*512 : offset+(i+1)*512] = True\n",
    "    return roc_auc_score(np.array(labels), np.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7402918198529411\n"
    }
   ],
   "source": [
    "print(calculate_auroc(model0, 0, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.3531709558823529\n"
    }
   ],
   "source": [
    "print(calculate_auroc(model1, 1, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the digit 1 has a smaller auroc value. (similiar result was shown by An and Cho)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpytorchvirtualenvd59703a1036943fe8cb882f2eefa042e",
   "display_name": "Python 3.6.10 64-bit ('pytorch': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
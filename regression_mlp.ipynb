{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import SGD\n",
    "from torch.nn import Linear, Sigmoid, ReLU, Module, MSELoss, ModuleList, Sequential, BatchNorm1d\n",
    "from torch.nn.init import xavier_uniform_ \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSV_Importer(Dataset):\n",
    "    #loader\n",
    "    def __init__(self, path):\n",
    "        #use pandas to load the csv file\n",
    "        df = read_csv(path, header=None)\n",
    "        #split the features and lables\n",
    "        self.X = df.values[:, :-1].astype('float32')\n",
    "        self.y = df.values[:, -1].astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    #returns the number of rows\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    #returns a given row\n",
    "    def __getitem__(self, i):\n",
    "        return [self.X[i], self.y[i]]\n",
    "\n",
    "    #splits train and test data\n",
    "    def get_splits(self, test_size=0.3):\n",
    "        test = round(test_size * len(self.X))\n",
    "        train = len(self.X) - test\n",
    "        return random_split(self, [train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    #initialize model\n",
    "    def __init__(self, n_inputs, layer_sizes, n_outputs):\n",
    "        super(MLP, self).__init__()\n",
    "        #Build the model\n",
    "        self.layers = []\n",
    "        layer = Linear(n_inputs, layer_sizes[0])\n",
    "        xavier_uniform_(layer.weight)\n",
    "        bn = BatchNorm1d(num_features=layer_sizes[0])\n",
    "        self.layers.extend([layer, bn, ReLU()])\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            xavier_uniform_(layer.weight)\n",
    "            bn = BatchNorm1d(num_features=layer_sizes[i+1])\n",
    "            self.layers.extend([layer, bn, ReLU()])\n",
    "        layer = Linear(layer_sizes[-1], n_outputs)\n",
    "        xavier_uniform_(layer.weight)\n",
    "        self.layers.append(layer)\n",
    "        self.layers = Sequential(*self.layers)\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, path, model, batch_train=64, batch_test=512):\n",
    "        #load data\n",
    "        self.dataset = CSV_Importer(path)\n",
    "        #split data\n",
    "        train, test = self.dataset.get_splits(test_size=0.33)\n",
    "        self.train_dl = DataLoader(train, batch_size=batch_train, shuffle=True)\n",
    "        self.test_dl = DataLoader(test, batch_size=batch_test, shuffle=False)\n",
    "        #save the model\n",
    "        self.model = model\n",
    "        self.batch_train = batch_train\n",
    "\n",
    "    def train(self, epochs):\n",
    "        self.model.train()\n",
    "        #set loss and optimizer\n",
    "        criterion = MSELoss()\n",
    "        optimizer = SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "        #train\n",
    "        losses = []\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "            #batches\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(self.train_dl):\n",
    "                inputs, labels = data\n",
    "                #initialize the gradients\n",
    "                optimizer.zero_grad()\n",
    "                #predict the output\n",
    "                y = self.model(inputs)\n",
    "                #calculate loss\n",
    "                loss = criterion(y, labels)\n",
    "                #backprop\n",
    "                loss.backward()\n",
    "                #update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            #print loss every epoch\n",
    "            tqdm.write(\"[%d] loss: %.4f\" % (epoch + 1, running_loss / self.batch_train))\n",
    "            losses.append(running_loss / self.batch_train)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        \n",
    "    \n",
    "    def evaluate(self):\n",
    "        predictions, true_labels = list(), list()\n",
    "        for i, (inputs, labels) in enumerate(self.test_dl):\n",
    "            #predict output\n",
    "            y = self.model(inputs)\n",
    "            y = y.detach().numpy()\n",
    "            #get labels\n",
    "            true_label = labels.numpy()\n",
    "            true_label = true_label.reshape((len(true_label), 1))\n",
    "            #save\n",
    "            predictions.append(y)\n",
    "            true_labels.append(true_label)\n",
    "        predictions, true_labels = np.vstack(predictions), np.vstack(true_labels)\n",
    "        #calculate loss\n",
    "        mse = mean_squared_error(true_labels, predictions)\n",
    "        return mse\n",
    "\n",
    "    def predict(self, x):\n",
    "        #create tensor\n",
    "        x = Tensor([x])\n",
    "        #predict output\n",
    "        self.model.eval()\n",
    "        y = self.model(x)\n",
    "        return y.detach().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339 167\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/boston_housing.csv\"\n",
    "model = MLP(13, [50, 50, 10], 1)\n",
    "trainer = Trainer(path, model)\n",
    "print(len(trainer.train_dl.dataset), len(trainer.test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097c9e8eb4e448a88d51655a9719de05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=500.0, style=ProgressStyle(description_widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 55.2229\n",
      "[2] loss: 52.4611\n",
      "[3] loss: 52.3792\n",
      "[4] loss: 53.8211\n",
      "[5] loss: 51.9499\n",
      "[6] loss: 51.1992\n",
      "[7] loss: 49.2642\n",
      "[8] loss: 48.9639\n",
      "[9] loss: 49.5458\n",
      "[10] loss: 52.6123\n",
      "[11] loss: 48.9628\n",
      "[12] loss: 47.8827\n",
      "[13] loss: 47.8508\n",
      "[14] loss: 49.2598\n",
      "[15] loss: 47.2007\n",
      "[16] loss: 47.8127\n",
      "[17] loss: 45.8699\n",
      "[18] loss: 47.6787\n",
      "[19] loss: 47.1878\n",
      "[20] loss: 46.9647\n",
      "[21] loss: 45.9483\n",
      "[22] loss: 44.7670\n",
      "[23] loss: 46.2131\n",
      "[24] loss: 43.4912\n",
      "[25] loss: 43.6907\n",
      "[26] loss: 46.2011\n",
      "[27] loss: 44.0309\n",
      "[28] loss: 43.5952\n",
      "[29] loss: 42.8137\n",
      "[30] loss: 43.3569\n",
      "[31] loss: 41.6522\n",
      "[32] loss: 40.6117\n",
      "[33] loss: 41.0659\n",
      "[34] loss: 39.1346\n",
      "[35] loss: 38.0748\n",
      "[36] loss: 40.4337\n",
      "[37] loss: 38.1391\n",
      "[38] loss: 38.7496\n",
      "[39] loss: 37.0462\n",
      "[40] loss: 36.9986\n",
      "[41] loss: 36.4784\n",
      "[42] loss: 36.4222\n",
      "[43] loss: 34.9517\n",
      "[44] loss: 35.7488\n",
      "[45] loss: 33.1455\n",
      "[46] loss: 34.6107\n",
      "[47] loss: 34.6340\n",
      "[48] loss: 32.4031\n",
      "[49] loss: 32.8602\n",
      "[50] loss: 30.6598\n",
      "[51] loss: 29.9351\n",
      "[52] loss: 30.3762\n",
      "[53] loss: 32.6746\n",
      "[54] loss: 28.9760\n",
      "[55] loss: 28.4770\n",
      "[56] loss: 28.2150\n",
      "[57] loss: 28.0747\n",
      "[58] loss: 26.3686\n",
      "[59] loss: 25.4555\n",
      "[60] loss: 24.6098\n",
      "[61] loss: 24.7569\n",
      "[62] loss: 24.5907\n",
      "[63] loss: 24.0886\n",
      "[64] loss: 22.1260\n",
      "[65] loss: 23.3393\n",
      "[66] loss: 21.9428\n",
      "[67] loss: 20.9121\n",
      "[68] loss: 20.2442\n",
      "[69] loss: 19.5947\n",
      "[70] loss: 19.8385\n",
      "[71] loss: 18.5597\n",
      "[72] loss: 18.7294\n",
      "[73] loss: 19.4721\n",
      "[74] loss: 17.1227\n",
      "[75] loss: 16.9121\n",
      "[76] loss: 16.7930\n",
      "[77] loss: 14.8944\n",
      "[78] loss: 15.3116\n",
      "[79] loss: 14.7777\n",
      "[80] loss: 13.8611\n",
      "[81] loss: 13.9039\n",
      "[82] loss: 13.1085\n",
      "[83] loss: 12.3923\n",
      "[84] loss: 12.1669\n",
      "[85] loss: 11.4237\n",
      "[86] loss: 10.6669\n",
      "[87] loss: 10.9856\n",
      "[88] loss: 9.8987\n",
      "[89] loss: 10.2372\n",
      "[90] loss: 9.5195\n",
      "[91] loss: 8.4935\n",
      "[92] loss: 8.7887\n",
      "[93] loss: 8.3731\n",
      "[94] loss: 8.0464\n",
      "[95] loss: 8.1704\n",
      "[96] loss: 7.6743\n",
      "[97] loss: 6.9632\n",
      "[98] loss: 7.9749\n",
      "[99] loss: 6.4250\n",
      "[100] loss: 6.3489\n",
      "[101] loss: 6.0636\n",
      "[102] loss: 5.7020\n",
      "[103] loss: 6.3818\n",
      "[104] loss: 5.9007\n",
      "[105] loss: 5.7311\n",
      "[106] loss: 5.4295\n",
      "[107] loss: 5.3483\n",
      "[108] loss: 5.8110\n",
      "[109] loss: 4.7708\n",
      "[110] loss: 5.9279\n",
      "[111] loss: 4.1489\n",
      "[112] loss: 3.7524\n",
      "[113] loss: 3.9276\n",
      "[114] loss: 3.9821\n",
      "[115] loss: 3.9473\n",
      "[116] loss: 3.6691\n",
      "[117] loss: 3.2729\n",
      "[118] loss: 3.1263\n",
      "[119] loss: 2.7503\n",
      "[120] loss: 3.0346\n",
      "[121] loss: 3.4225\n",
      "[122] loss: 2.8437\n",
      "[123] loss: 2.4874\n",
      "[124] loss: 2.5513\n",
      "[125] loss: 2.4939\n",
      "[126] loss: 2.6668\n",
      "[127] loss: 2.6604\n",
      "[128] loss: 3.2172\n",
      "[129] loss: 2.9230\n",
      "[130] loss: 2.8470\n",
      "[131] loss: 2.7775\n",
      "[132] loss: 2.9565\n",
      "[133] loss: 2.9179\n",
      "[134] loss: 2.8862\n",
      "[135] loss: 2.6629\n",
      "[136] loss: 2.2971\n",
      "[137] loss: 2.1659\n",
      "[138] loss: 2.4252\n",
      "[139] loss: 2.2798\n",
      "[140] loss: 2.4571\n",
      "[141] loss: 2.8356\n",
      "[142] loss: 2.5996\n",
      "[143] loss: 2.6236\n",
      "[144] loss: 2.2516\n",
      "[145] loss: 2.6914\n",
      "[146] loss: 2.5835\n",
      "[147] loss: 2.1972\n",
      "[148] loss: 2.4392\n",
      "[149] loss: 1.9193\n",
      "[150] loss: 2.0662\n",
      "[151] loss: 1.8630\n",
      "[152] loss: 2.6057\n",
      "[153] loss: 2.0874\n",
      "[154] loss: 1.9125\n",
      "[155] loss: 1.6640\n",
      "[156] loss: 1.8430\n",
      "[157] loss: 1.6131\n",
      "[158] loss: 1.7915\n",
      "[159] loss: 1.8864\n",
      "[160] loss: 1.9453\n",
      "[161] loss: 1.6655\n",
      "[162] loss: 1.6878\n",
      "[163] loss: 1.9859\n",
      "[164] loss: 2.0577\n",
      "[165] loss: 1.3827\n",
      "[166] loss: 2.5869\n",
      "[167] loss: 1.7286\n",
      "[168] loss: 2.3139\n",
      "[169] loss: 1.8061\n",
      "[170] loss: 1.9194\n",
      "[171] loss: 2.1208\n",
      "[172] loss: 1.6759\n",
      "[173] loss: 1.5845\n",
      "[174] loss: 1.6660\n",
      "[175] loss: 1.8344\n",
      "[176] loss: 1.7021\n",
      "[177] loss: 1.5490\n",
      "[178] loss: 2.0759\n",
      "[179] loss: 1.6819\n",
      "[180] loss: 1.6598\n",
      "[181] loss: 1.7288\n",
      "[182] loss: 1.4922\n",
      "[183] loss: 1.4432\n",
      "[184] loss: 1.5725\n",
      "[185] loss: 1.4021\n",
      "[186] loss: 1.7188\n",
      "[187] loss: 1.5902\n",
      "[188] loss: 1.6444\n",
      "[189] loss: 2.2793\n",
      "[190] loss: 1.3873\n",
      "[191] loss: 1.7968\n",
      "[192] loss: 1.6871\n",
      "[193] loss: 2.0847\n",
      "[194] loss: 1.8587\n",
      "[195] loss: 1.8215\n",
      "[196] loss: 1.7276\n",
      "[197] loss: 1.8930\n",
      "[198] loss: 1.6655\n",
      "[199] loss: 1.8702\n",
      "[200] loss: 2.0168\n",
      "[201] loss: 2.7952\n",
      "[202] loss: 1.7606\n",
      "[203] loss: 1.5248\n",
      "[204] loss: 1.7425\n",
      "[205] loss: 1.9363\n",
      "[206] loss: 1.8069\n",
      "[207] loss: 1.8860\n",
      "[208] loss: 1.6251\n",
      "[209] loss: 1.8481\n",
      "[210] loss: 1.4577\n",
      "[211] loss: 1.5091\n",
      "[212] loss: 1.4608\n",
      "[213] loss: 1.6386\n",
      "[214] loss: 1.7693\n",
      "[215] loss: 1.4836\n",
      "[216] loss: 1.4146\n",
      "[217] loss: 1.4864\n",
      "[218] loss: 1.4123\n",
      "[219] loss: 1.2303\n",
      "[220] loss: 1.6826\n",
      "[221] loss: 1.6827\n",
      "[222] loss: 1.4566\n",
      "[223] loss: 1.3434\n",
      "[224] loss: 1.5310\n",
      "[225] loss: 1.4131\n",
      "[226] loss: 1.5002\n",
      "[227] loss: 1.1944\n",
      "[228] loss: 1.7405\n",
      "[229] loss: 1.7254\n",
      "[230] loss: 1.6352\n",
      "[231] loss: 1.5548\n",
      "[232] loss: 1.5338\n",
      "[233] loss: 1.4352\n",
      "[234] loss: 1.9464\n",
      "[235] loss: 1.4125\n",
      "[236] loss: 1.6505\n",
      "[237] loss: 1.8924\n",
      "[238] loss: 1.5642\n",
      "[239] loss: 1.8272\n",
      "[240] loss: 1.8959\n",
      "[241] loss: 3.0530\n",
      "[242] loss: 1.9522\n",
      "[243] loss: 1.7583\n",
      "[244] loss: 1.8160\n",
      "[245] loss: 1.6794\n",
      "[246] loss: 1.3345\n",
      "[247] loss: 1.7311\n",
      "[248] loss: 1.6403\n",
      "[249] loss: 1.4349\n",
      "[250] loss: 1.8877\n",
      "[251] loss: 1.4688\n",
      "[252] loss: 1.3639\n",
      "[253] loss: 1.6422\n",
      "[254] loss: 1.5895\n",
      "[255] loss: 1.6763\n",
      "[256] loss: 1.6808\n",
      "[257] loss: 1.7501\n",
      "[258] loss: 1.5533\n",
      "[259] loss: 1.9395\n",
      "[260] loss: 1.4214\n",
      "[261] loss: 1.3141\n",
      "[262] loss: 1.2646\n",
      "[263] loss: 1.4441\n",
      "[264] loss: 1.1983\n",
      "[265] loss: 1.3427\n",
      "[266] loss: 1.4823\n",
      "[267] loss: 1.7025\n",
      "[268] loss: 2.0484\n",
      "[269] loss: 2.3722\n",
      "[270] loss: 1.6364\n",
      "[271] loss: 1.5174\n",
      "[272] loss: 1.6466\n",
      "[273] loss: 1.3008\n",
      "[274] loss: 1.3941\n",
      "[275] loss: 1.1585\n",
      "[276] loss: 1.1787\n",
      "[277] loss: 1.4454\n",
      "[278] loss: 1.4749\n",
      "[279] loss: 1.4460\n",
      "[280] loss: 1.2645\n",
      "[281] loss: 1.2035\n",
      "[282] loss: 1.8580\n",
      "[283] loss: 1.4976\n",
      "[284] loss: 1.4624\n",
      "[285] loss: 1.3733\n",
      "[286] loss: 1.3661\n",
      "[287] loss: 1.2359\n",
      "[288] loss: 1.4112\n",
      "[289] loss: 1.5176\n",
      "[290] loss: 1.1659\n",
      "[291] loss: 1.2541\n",
      "[292] loss: 1.2794\n",
      "[293] loss: 2.1348\n",
      "[294] loss: 1.4786\n",
      "[295] loss: 1.2217\n",
      "[296] loss: 1.1801\n",
      "[297] loss: 1.3349\n",
      "[298] loss: 1.1566\n",
      "[299] loss: 1.1416\n",
      "[300] loss: 1.6544\n",
      "[301] loss: 1.7920\n",
      "[302] loss: 1.4729\n",
      "[303] loss: 1.7539\n",
      "[304] loss: 2.1581\n",
      "[305] loss: 1.4335\n",
      "[306] loss: 1.6789\n",
      "[307] loss: 1.2187\n",
      "[308] loss: 1.6367\n",
      "[309] loss: 1.4160\n",
      "[310] loss: 1.3049\n",
      "[311] loss: 1.2105\n",
      "[312] loss: 1.5754\n",
      "[313] loss: 1.3245\n",
      "[314] loss: 1.4191\n",
      "[315] loss: 1.5410\n",
      "[316] loss: 1.3542\n",
      "[317] loss: 1.5310\n",
      "[318] loss: 1.3223\n",
      "[319] loss: 1.5363\n",
      "[320] loss: 1.4490\n",
      "[321] loss: 1.6292\n",
      "[322] loss: 1.2059\n",
      "[323] loss: 1.4421\n",
      "[324] loss: 1.2756\n",
      "[325] loss: 1.2780\n",
      "[326] loss: 1.6374\n",
      "[327] loss: 1.5706\n",
      "[328] loss: 1.3362\n",
      "[329] loss: 1.3989\n",
      "[330] loss: 1.2543\n",
      "[331] loss: 1.5637\n",
      "[332] loss: 1.8637\n",
      "[333] loss: 1.4818\n",
      "[334] loss: 1.3636\n",
      "[335] loss: 1.2158\n",
      "[336] loss: 1.1993\n",
      "[337] loss: 1.6697\n",
      "[338] loss: 1.3619\n",
      "[339] loss: 1.3696\n",
      "[340] loss: 1.0803\n",
      "[341] loss: 1.6549\n",
      "[342] loss: 1.4567\n",
      "[343] loss: 1.1537\n",
      "[344] loss: 1.1107\n",
      "[345] loss: 1.0612\n",
      "[346] loss: 1.6809\n",
      "[347] loss: 1.2020\n",
      "[348] loss: 1.4421\n",
      "[349] loss: 1.0202\n",
      "[350] loss: 1.7381\n",
      "[351] loss: 1.1511\n",
      "[352] loss: 1.2734\n",
      "[353] loss: 1.9533\n",
      "[354] loss: 1.0932\n",
      "[355] loss: 1.2849\n",
      "[356] loss: 1.4056\n",
      "[357] loss: 1.5741\n",
      "[358] loss: 1.0980\n",
      "[359] loss: 1.2792\n",
      "[360] loss: 1.2151\n",
      "[361] loss: 1.4272\n",
      "[362] loss: 1.3603\n",
      "[363] loss: 1.1295\n",
      "[364] loss: 1.5472\n",
      "[365] loss: 1.5971\n",
      "[366] loss: 1.6043\n",
      "[367] loss: 1.4848\n",
      "[368] loss: 1.2975\n",
      "[369] loss: 1.3483\n",
      "[370] loss: 1.3534\n",
      "[371] loss: 1.4750\n",
      "[372] loss: 1.7037\n",
      "[373] loss: 1.3697\n",
      "[374] loss: 1.3610\n",
      "[375] loss: 1.5486\n",
      "[376] loss: 2.0446\n",
      "[377] loss: 1.2691\n",
      "[378] loss: 1.7742\n",
      "[379] loss: 1.3190\n",
      "[380] loss: 1.5162\n",
      "[381] loss: 1.4756\n",
      "[382] loss: 1.5637\n",
      "[383] loss: 1.1354\n",
      "[384] loss: 1.8374\n",
      "[385] loss: 1.3509\n",
      "[386] loss: 1.2798\n",
      "[387] loss: 0.9556\n",
      "[388] loss: 1.2910\n",
      "[389] loss: 2.8356\n",
      "[390] loss: 1.4885\n",
      "[391] loss: 1.9580\n",
      "[392] loss: 1.6055\n",
      "[393] loss: 1.2202\n",
      "[394] loss: 1.2527\n",
      "[395] loss: 1.3123\n",
      "[396] loss: 1.0664\n",
      "[397] loss: 1.4291\n",
      "[398] loss: 1.2135\n",
      "[399] loss: 1.1221\n",
      "[400] loss: 1.0954\n",
      "[401] loss: 1.2156\n",
      "[402] loss: 1.6147\n",
      "[403] loss: 1.3273\n",
      "[404] loss: 1.5069\n",
      "[405] loss: 1.5880\n",
      "[406] loss: 1.3517\n",
      "[407] loss: 1.0481\n",
      "[408] loss: 1.1554\n",
      "[409] loss: 1.1252\n",
      "[410] loss: 1.6939\n",
      "[411] loss: 1.2831\n",
      "[412] loss: 1.2562\n",
      "[413] loss: 1.0474\n",
      "[414] loss: 1.3067\n",
      "[415] loss: 1.2326\n",
      "[416] loss: 0.9657\n",
      "[417] loss: 1.0140\n",
      "[418] loss: 1.2501\n",
      "[419] loss: 1.4116\n",
      "[420] loss: 1.2961\n",
      "[421] loss: 1.4091\n",
      "[422] loss: 1.3799\n",
      "[423] loss: 0.9592\n",
      "[424] loss: 1.2994\n",
      "[425] loss: 1.0610\n",
      "[426] loss: 1.2840\n",
      "[427] loss: 1.6612\n",
      "[428] loss: 1.1508\n",
      "[429] loss: 1.0394\n",
      "[430] loss: 1.4381\n",
      "[431] loss: 1.2635\n",
      "[432] loss: 1.2674\n",
      "[433] loss: 1.2099\n",
      "[434] loss: 1.2825\n",
      "[435] loss: 1.4132\n",
      "[436] loss: 1.3771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[437] loss: 1.0511\n",
      "[438] loss: 1.3331\n",
      "[439] loss: 0.9407\n",
      "[440] loss: 1.0173\n",
      "[441] loss: 1.0488\n",
      "[442] loss: 1.2443\n",
      "[443] loss: 1.2699\n",
      "[444] loss: 1.0740\n",
      "[445] loss: 1.3510\n",
      "[446] loss: 0.9862\n",
      "[447] loss: 1.4206\n",
      "[448] loss: 0.9830\n",
      "[449] loss: 1.0357\n",
      "[450] loss: 1.2112\n",
      "[451] loss: 1.5902\n",
      "[452] loss: 1.1918\n",
      "[453] loss: 1.1854\n",
      "[454] loss: 0.9608\n",
      "[455] loss: 1.0471\n",
      "[456] loss: 1.1729\n",
      "[457] loss: 1.7006\n",
      "[458] loss: 1.2588\n",
      "[459] loss: 1.2409\n",
      "[460] loss: 1.2107\n",
      "[461] loss: 1.0787\n",
      "[462] loss: 1.4229\n",
      "[463] loss: 1.1171\n",
      "[464] loss: 0.9219\n",
      "[465] loss: 1.0869\n",
      "[466] loss: 1.3915\n",
      "[467] loss: 1.0126\n",
      "[468] loss: 1.4957\n",
      "[469] loss: 1.9904\n",
      "[470] loss: 1.3741\n",
      "[471] loss: 1.4202\n",
      "[472] loss: 1.1393\n",
      "[473] loss: 1.2768\n",
      "[474] loss: 1.3784\n",
      "[475] loss: 1.4043\n",
      "[476] loss: 1.1282\n",
      "[477] loss: 1.1021\n",
      "[478] loss: 1.4293\n",
      "[479] loss: 1.5497\n",
      "[480] loss: 1.1424\n",
      "[481] loss: 1.3048\n",
      "[482] loss: 1.0180\n",
      "[483] loss: 1.4020\n",
      "[484] loss: 2.1035\n",
      "[485] loss: 1.3049\n",
      "[486] loss: 1.2299\n",
      "[487] loss: 1.2299\n",
      "[488] loss: 1.0440\n",
      "[489] loss: 1.1903\n",
      "[490] loss: 1.0010\n",
      "[491] loss: 1.3579\n",
      "[492] loss: 1.0468\n",
      "[493] loss: 1.1510\n",
      "[494] loss: 0.9888\n",
      "[495] loss: 0.9979\n",
      "[496] loss: 1.0830\n",
      "[497] loss: 1.2943\n",
      "[498] loss: 1.2880\n",
      "[499] loss: 1.3044\n",
      "[500] loss: 1.0769\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 19.9254, RMSE: 4.4638\n",
      "Predicted: 35.1578\n"
     ]
    }
   ],
   "source": [
    "mse = trainer.evaluate()\n",
    "print('MSE: %.4f, RMSE: %.4f' % (mse, np.sqrt(mse)))\n",
    "x = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "y = trainer.predict(x)\n",
    "\n",
    "print(\"Predicted: %.4f\" % y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch': virtualenv)",
   "language": "python",
   "name": "python361064bitpytorchvirtualenvd59703a1036943fe8cb882f2eefa042e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
